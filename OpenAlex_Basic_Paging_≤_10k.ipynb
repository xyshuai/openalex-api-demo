{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzrxvWfaCdZ6oZbmQcpUAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xyshuai/openalex-api-examples/blob/main/OpenAlex_Basic_Paging_%E2%89%A4_10k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQLpd5-z8Jrz"
      },
      "outputs": [],
      "source": [
        "# === OpenAlex Basic Paging (up to 10,000 records) ===\n",
        "# - Uses classic page-based pagination: page=1,2,...,MAX_PAGES\n",
        "# - Collects results into a single CSV\n",
        "# - Designed to run in Google Colab, but also works in a normal Python environment with minor adjustment to the file path.\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "# 1) Try to mount Google Drive (if in Colab); if it fails, fall back to a local folder\n",
        "USE_DRIVE = False\n",
        "try:\n",
        "    from google.colab import drive, files  # type: ignore\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        USE_DRIVE = True\n",
        "        save_dir = \"/content/drive/MyDrive/OpenAlex\"\n",
        "        print(\"‚úÖ Google Drive mounted. Saving to:\", save_dir)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Failed to mount Google Drive. Falling back to /content. Error:\", e)\n",
        "        save_dir = \"/content\"\n",
        "except Exception:\n",
        "    # Not running in Colab; save to the current working directory\n",
        "    save_dir = \".\"\n",
        "    print(\"‚ÑπÔ∏è Not running in Colab. Saving to the current directory.\")\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "out_csv = os.path.join(save_dir, \"openalex_basic_paging_full.csv\")\n",
        "\n",
        "# 2) OpenAlex API configuration (basic paging)\n",
        "BASE = \"https://api.openalex.org/works\"\n",
        "\n",
        "# Core filtering conditions\n",
        "# - open_access.is_oa:true  ‚Üí only open-access works\n",
        "# - authorships.countries:countries/cn ‚Üí at least one author with country code \"MY\"\n",
        "# - publication_year:2023-2024 ‚Üí works published between 2023 and 2024\n",
        "# - type: article or review\n",
        "FILTERS = [\n",
        "    \"open_access.is_oa:true\",\n",
        "    \"authorships.countries:countries/my\",\n",
        "    \"publication_year:2023-2024\",\n",
        "    \"type:types/article|types/review\",\n",
        "]\n",
        "\n",
        "# Paging settings:\n",
        "# - per_page ‚â§ 200 (OpenAlex limit)\n",
        "# - MAX_PAGES ‚â§ 50 ‚Üí at most 10,000 results (200 * 50)\n",
        "PER_PAGE = 200\n",
        "MAX_PAGES = 50\n",
        "\n",
        "PARAMS_BASE = {\n",
        "    \"filter\": \",\".join(FILTERS),\n",
        "    \"sort\": \"cited_by_count:desc\",\n",
        "    \"per_page\": PER_PAGE,\n",
        "    \"mailto\": \"your.name@domain.com\",  # TODO: replace with your email\n",
        "}\n",
        "\n",
        "# ---------- Helper functions ----------\n",
        "\n",
        "def flatten_authors(authorships):\n",
        "    \"\"\"Return a simple ';'-separated string of author names.\"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    names = []\n",
        "    for a in authorships:\n",
        "        nm = (a.get(\"author\") or {}).get(\"display_name\")\n",
        "        if nm:\n",
        "            names.append(nm)\n",
        "    return \"; \".join(names)\n",
        "\n",
        "def flatten_authors_affiliations(authorships):\n",
        "    \"\"\"\n",
        "    Return a string such as:\n",
        "        'Author A (Affil1;Affil2); Author B (AffilX)'\n",
        "    If no affiliations exist, only the author name is shown.\n",
        "    \"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for a in authorships:\n",
        "        nm = (a.get(\"author\") or {}).get(\"display_name\") or \"\"\n",
        "        affs = []\n",
        "        for inst in (a.get(\"institutions\") or []):\n",
        "            dn = inst.get(\"display_name\") or \"\"\n",
        "            if dn:\n",
        "                affs.append(dn)\n",
        "        if nm:\n",
        "            if affs:\n",
        "                parts.append(f\"{nm} ({';'.join(affs)})\")\n",
        "            else:\n",
        "                parts.append(nm)\n",
        "    return \"; \".join(parts)\n",
        "\n",
        "def top_concepts(concepts, n=3):\n",
        "    \"\"\"\n",
        "    Return the top-n concepts by score as a ';'-separated string of concept names.\n",
        "    \"\"\"\n",
        "    if not concepts:\n",
        "        return \"\"\n",
        "    ranked = sorted(concepts, key=lambda c: c.get(\"score\", 0), reverse=True)[:n]\n",
        "    return \"; \".join([c.get(\"display_name\", \"\") for c in ranked if c.get(\"display_name\")])\n",
        "\n",
        "def topic_label(t):\n",
        "    \"\"\"Format one topic as 'Name (score=0.87)'.\"\"\"\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    name = t.get(\"display_name\", \"\")\n",
        "    sc = t.get(\"score\", None)\n",
        "    if sc is not None:\n",
        "        return f\"{name} (score={sc:.2f})\"\n",
        "    return name\n",
        "\n",
        "def collect_first_author_country_codes(authorships):\n",
        "    \"\"\"\n",
        "    Collect country codes for the first author:\n",
        "    - authorships[0].countries[]\n",
        "    - authorships[0].institutions[].country_code\n",
        "    \"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    a0 = authorships[0]\n",
        "    s = set()\n",
        "    for c in (a0.get(\"countries\") or []):\n",
        "        if c:\n",
        "            s.add(c)\n",
        "    for inst in (a0.get(\"institutions\") or []):\n",
        "        cc = inst.get(\"country_code\")\n",
        "        if cc:\n",
        "            s.add(cc)\n",
        "    return \";\".join(sorted(s)) if s else \"\"\n",
        "\n",
        "def collect_institution_country_codes(work):\n",
        "    \"\"\"\n",
        "    Collect country codes from the top-level institutions[] array.\n",
        "    \"\"\"\n",
        "    s = set()\n",
        "    for inst in (work.get(\"institutions\") or []):\n",
        "        cc = inst.get(\"country_code\")\n",
        "        if cc:\n",
        "            s.add(cc)\n",
        "    return \";\".join(sorted(s)) if s else \"\"\n",
        "\n",
        "def pick_source_info(work):\n",
        "    \"\"\"\n",
        "    Try to infer the journal/source information using:\n",
        "    1) host_venue\n",
        "    2) primary_location.source\n",
        "    3) locations[].source (first available)\n",
        "    Returns: (journal_name, issn_l)\n",
        "    \"\"\"\n",
        "    journal = \"\"\n",
        "    issn_l = \"\"\n",
        "\n",
        "    host = work.get(\"host_venue\") or {}\n",
        "    journal = host.get(\"display_name\") or \"\"\n",
        "    issn_l = host.get(\"issn_l\") or \"\"\n",
        "\n",
        "    if not journal:\n",
        "        pl = work.get(\"primary_location\") or {}\n",
        "        src = pl.get(\"source\") or {}\n",
        "        journal = journal or src.get(\"display_name\") or \"\"\n",
        "        issn_l = issn_l or src.get(\"issn_l\") or \"\"\n",
        "\n",
        "    if not journal:\n",
        "        for loc in (work.get(\"locations\") or []):\n",
        "            src = loc.get(\"source\") or {}\n",
        "            if src.get(\"display_name\"):\n",
        "                journal = journal or src.get(\"display_name\")\n",
        "                issn_l = issn_l or src.get(\"issn_l\") or \"\"\n",
        "                break\n",
        "\n",
        "    return journal or \"\", issn_l or \"\"\n",
        "\n",
        "def get_corresponding_authors(authorships):\n",
        "    \"\"\"Return a ';'-separated list of corresponding author names.\"\"\"\n",
        "    if not authorships:\n",
        "        return \"\"\n",
        "    names = []\n",
        "    for a in authorships:\n",
        "        if a.get(\"is_corresponding\"):\n",
        "            nm = (a.get(\"author\") or {}).get(\"display_name\")\n",
        "            if nm:\n",
        "                names.append(nm)\n",
        "    return \";\".join(names) if names else \"\"\n",
        "\n",
        "def get_fwci_and_percentile(work):\n",
        "    \"\"\"\n",
        "    Extract FWCI and citation-normalized percentile information.\n",
        "    Returns (fwci, citation_percentile, top_1pct_flag, top_10pct_flag)\n",
        "    \"\"\"\n",
        "    fwci = work.get(\"fwci\", \"\")\n",
        "    if not isinstance(fwci, (int, float, str)):\n",
        "        fwci = \"\"\n",
        "\n",
        "    cnp = work.get(\"citation_normalized_percentile\") or {}\n",
        "    perc = cnp.get(\"value\", None)\n",
        "    citation_percentile = f\"{perc:.6f}\" if isinstance(perc, (int, float)) else \"\"\n",
        "\n",
        "    top1 = cnp.get(\"is_in_top_1_percent\") or cnp.get(\"is_in_top1_percent\")\n",
        "    top10 = cnp.get(\"is_in_top_10_percent\") or cnp.get(\"is_in_top10_percent\")\n",
        "\n",
        "    citation_top_1pct = \"Yes\" if top1 else (\"No\" if top1 is not None else \"\")\n",
        "    citation_top_10pct = \"Yes\" if top10 else (\"No\" if top10 is not None else \"\")\n",
        "\n",
        "    return str(fwci), citation_percentile, citation_top_1pct, citation_top_10pct\n",
        "\n",
        "def parse_apc_list(work):\n",
        "    \"\"\"\n",
        "    Normalize the 'apc_list' field into a readable string.\n",
        "    Handles list/dict/scalar/None and returns something like:\n",
        "        '2000 USD; 1500 EUR'\n",
        "    \"\"\"\n",
        "    apc = work.get(\"apc_list\", None)\n",
        "    items = []\n",
        "\n",
        "    def norm_one(x):\n",
        "        if isinstance(x, dict):\n",
        "            val = x.get(\"value\", None)\n",
        "            cur = x.get(\"currency\", \"\")\n",
        "            if val is not None and cur:\n",
        "                return f\"{val} {cur}\"\n",
        "            if val is not None:\n",
        "                return str(val)\n",
        "            if cur:\n",
        "                return cur\n",
        "            return str(x)\n",
        "        if isinstance(x, (int, float)):\n",
        "            return str(x)\n",
        "        if isinstance(x, str):\n",
        "            return x.strip()\n",
        "        return str(x)\n",
        "\n",
        "    if isinstance(apc, list):\n",
        "        for it in apc:\n",
        "            s = norm_one(it)\n",
        "            if s:\n",
        "                items.append(s)\n",
        "    elif isinstance(apc, dict):\n",
        "        s = norm_one(apc)\n",
        "        if s:\n",
        "            items.append(s)\n",
        "    elif isinstance(apc, (str, int, float)):\n",
        "        s = norm_one(apc)\n",
        "        if s:\n",
        "            items.append(s)\n",
        "\n",
        "    return \"; \".join(items)\n",
        "\n",
        "# 3) Download all pages (basic paging)\n",
        "all_results = []\n",
        "total_downloaded = 0\n",
        "\n",
        "for page in range(1, MAX_PAGES + 1):\n",
        "    params = PARAMS_BASE.copy()\n",
        "    params[\"page\"] = page\n",
        "\n",
        "    print(f\"‚ñ∂Ô∏è Requesting page {page} ...\")\n",
        "    resp = requests.get(BASE, params=params, timeout=60)\n",
        "    if resp.status_code != 200:\n",
        "        print(f\"‚ö†Ô∏è HTTP {resp.status_code} on page {page}: {resp.text[:200]}\")\n",
        "        break\n",
        "\n",
        "    payload = resp.json()\n",
        "    results = payload.get(\"results\", [])\n",
        "\n",
        "    if not results:\n",
        "        print(f\"‚úÖ No more results at page {page}. Stopping.\")\n",
        "        break\n",
        "\n",
        "    all_results.extend(results)\n",
        "    total_downloaded += len(results)\n",
        "    print(f\"   Page {page}: {len(results)} records, total={total_downloaded}\")\n",
        "\n",
        "    # If this page returned fewer than PER_PAGE results, it is probably the last page\n",
        "    if len(results) < PER_PAGE:\n",
        "        print(\"‚ÑπÔ∏è Fewer results than 'per_page'; likely reached the final page.\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nüì• Finished downloading. Total records collected: {len(all_results)}\")\n",
        "\n",
        "# 4) Write to CSV\n",
        "out_headers = [\n",
        "    \"openalex_id\", \"doi\", \"title\", \"year\", \"type\", \"language\",\n",
        "    \"cited_by_count\", \"journal\", \"issn_l\",\n",
        "    \"is_oa\", \"oa_status\", \"oa_url\", \"license\", \"version\",\n",
        "    \"first_author\", \"authors_affiliations\", \"top3_concepts\",\n",
        "    \"primary_topic_id\", \"primary_topic_name\",\n",
        "    \"primary_topic_domain\", \"primary_topic_field\", \"primary_topic_subfield\",\n",
        "    \"topics_top5\",\n",
        "    # Countries\n",
        "    \"first_author_country_codes\", \"institution_country_codes\",\n",
        "    # Corresponding authors\n",
        "    \"corresponding_authors\",\n",
        "    # APC\n",
        "    \"apc_list_values\",\n",
        "    # Citation metrics\n",
        "    \"fwci\", \"citation_percentile\", \"citation_top_1pct\", \"citation_top_10pct\"\n",
        "]\n",
        "\n",
        "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(out_headers)\n",
        "\n",
        "    for wobj in all_results:\n",
        "        ids = wobj.get(\"ids\") or {}\n",
        "        doi = ids.get(\"doi\", \"\")\n",
        "        title = wobj.get(\"title\") or wobj.get(\"display_name\", \"\")\n",
        "        year = wobj.get(\"publication_year\", \"\")\n",
        "        wtype = wobj.get(\"type\", \"\")\n",
        "        lang = wobj.get(\"language\", \"\")\n",
        "        cited = wobj.get(\"cited_by_count\", 0)\n",
        "        journal, issn_l = pick_source_info(wobj)\n",
        "\n",
        "        oa = wobj.get(\"open_access\") or {}\n",
        "        is_oa = \"Yes\" if oa.get(\"is_oa\") else \"No\"\n",
        "        oa_status = oa.get(\"oa_status\", \"\")\n",
        "        oa_url = oa.get(\"oa_url\", \"\")\n",
        "\n",
        "        boa = wobj.get(\"best_oa_location\") or {}\n",
        "        license_ = boa.get(\"license\", \"\")\n",
        "        version = boa.get(\"version\", \"\")\n",
        "\n",
        "        authorships = wobj.get(\"authorships\", [])\n",
        "        first_author = authorships[0].get(\"author\", {}).get(\"display_name\") if authorships else \"\"\n",
        "        authors_aff = flatten_authors_affiliations(authorships)\n",
        "        concepts = wobj.get(\"concepts\", [])\n",
        "        top3 = top_concepts(concepts)\n",
        "\n",
        "        primary = wobj.get(\"primary_topic\") or {}\n",
        "        topics_list = wobj.get(\"topics\") or []\n",
        "        primary_topic_id = primary.get(\"id\", \"\")\n",
        "        primary_topic_name = primary.get(\"display_name\", \"\")\n",
        "        primary_topic_domain = (primary.get(\"domain\") or {}).get(\"display_name\", \"\")\n",
        "        primary_topic_field = (primary.get(\"field\") or {}).get(\"display_name\", \"\")\n",
        "        primary_topic_subfield = (primary.get(\"subfield\") or {}).get(\"display_name\", \"\")\n",
        "\n",
        "        others = [t for t in topics_list if (t.get(\"id\") != primary_topic_id)]\n",
        "        others_sorted = sorted(others, key=lambda t: t.get(\"score\", 0), reverse=True)[:5]\n",
        "        topics_top5 = \"; \".join([topic_label(t) for t in others_sorted])\n",
        "\n",
        "        first_author_cc = collect_first_author_country_codes(authorships)\n",
        "        inst_cc = collect_institution_country_codes(wobj)\n",
        "        corresponding_authors = get_corresponding_authors(authorships)\n",
        "        apc_list_values = parse_apc_list(wobj)\n",
        "        fwci, citation_percentile, top1, top10 = get_fwci_and_percentile(wobj)\n",
        "\n",
        "        w.writerow([\n",
        "            wobj.get(\"id\", \"\"), doi, title, year, wtype, lang,\n",
        "            cited, journal, issn_l,\n",
        "            is_oa, oa_status, oa_url, license_, version,\n",
        "            first_author, authors_aff, top3,\n",
        "            primary_topic_id, primary_topic_name,\n",
        "            primary_topic_domain, primary_topic_field, primary_topic_subfield,\n",
        "            topics_top5,\n",
        "            first_author_cc, inst_cc,\n",
        "            corresponding_authors,\n",
        "            apc_list_values,\n",
        "            fwci, citation_percentile, top1, top10\n",
        "        ])\n",
        "\n",
        "print(f\"\\n‚úÖ Export completed: {len(all_results)} records written.\")\n",
        "print(\"üìÑ CSV file saved at:\", out_csv)\n",
        "\n",
        "# Optional: if not using Drive in Colab, automatically trigger a download\n",
        "try:\n",
        "    if not USE_DRIVE:\n",
        "        from google.colab import files  # type: ignore\n",
        "        files.download(out_csv)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    }
  ]
}